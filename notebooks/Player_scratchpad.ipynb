{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from poke_env.player_configuration import PlayerConfiguration\n",
    "from poke_env.player.random_player import RandomPlayer\n",
    "from poke_env.player.baselines import SimpleHeuristicsPlayer\n",
    "from poke_env.server_configuration import ShowdownServerConfiguration\n",
    "from poke_env.player.utils import cross_evaluate\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge A simple Heuristic Player on Showdown for fun\n",
    "player = SimpleHeuristicsPlayer(\n",
    "    player_configuration=PlayerConfiguration(\"JoeNextLine\", \"underground\"),\n",
    "    server_configuration=ShowdownServerConfiguration,\n",
    ")\n",
    "\n",
    "await player.send_challenges(\"meatout\", n_challenges=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\nacha\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dotmap import DotMap \n",
    "\n",
    "\n",
    "from pokebot.bots.state_engine import SimpleStateEngine, HeuristicStateEngine\n",
    "from pokebot.bots.bot import BotPlayer\n",
    "from pokebot.models.dqn import Model\n",
    "from pokebot.trainers.trainer import SimpleDQNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DotMap(attr='eps', value_max=1.0, value_min=0.2, value_test=0, nb_steps=500000)\n"
     ]
    }
   ],
   "source": [
    "# Load hparams from file, this is where you would specify DQNAgent and Policy hyperparamters.\n",
    "hparams = DotMap(json.load(open('./hparams.json', 'r')))\n",
    "p_dict = hparams.policy\n",
    "a_dict = hparams.agent\n",
    "\n",
    "print((p_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = BotPlayer(\n",
    "    player_configuration=PlayerConfiguration(\"test2\",None),\n",
    "#     log_level=10,\n",
    "    state_engine=HeuristicStateEngine()   #Change StateEngine here, (Demo SimpleStateEngine(10))\n",
    "    )\n",
    "\n",
    "model = Model(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SimpleDQNTrainer(player, model, p_dict, a_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: -1.4811\n",
      "356 episodes - episode_reward: -41.577 [-48.000, 32.514] - loss: 0.019 - mae: 0.391 - mean_q: 0.523 - mean_eps: 0.991\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: -1.5023\n",
      "359 episodes - episode_reward: -41.870 [-47.990, 32.491] - loss: 0.019 - mae: 0.404 - mean_q: 0.429 - mean_eps: 0.976\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.4863\n",
      "361 episodes - episode_reward: -41.157 [-47.960, 34.982] - loss: 0.019 - mae: 0.430 - mean_q: 0.340 - mean_eps: 0.960\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.4591\n",
      "358 episodes - episode_reward: -40.756 [-48.000, 37.624] - loss: 0.019 - mae: 0.441 - mean_q: 0.402 - mean_eps: 0.944\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: -1.5053\n",
      "367 episodes - episode_reward: -41.029 [-48.000, 33.000] - loss: 0.019 - mae: 0.461 - mean_q: 0.360 - mean_eps: 0.928\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.5154\n",
      "370 episodes - episode_reward: -40.955 [-48.000, 33.000] - loss: 0.019 - mae: 0.522 - mean_q: 0.376 - mean_eps: 0.912\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.4906\n",
      "363 episodes - episode_reward: -41.048 [-47.990, 37.497] - loss: 0.019 - mae: 0.500 - mean_q: 0.317 - mean_eps: 0.896\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.4353\n",
      "360 episodes - episode_reward: -39.875 [-48.000, 40.760] - loss: 0.019 - mae: 0.536 - mean_q: 0.260 - mean_eps: 0.880\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.4390\n",
      "352 episodes - episode_reward: -40.913 [-48.000, 43.975] - loss: 0.019 - mae: 0.532 - mean_q: 0.279 - mean_eps: 0.864\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: -1.3950\n",
      "348 episodes - episode_reward: -40.063 [-48.000, 35.185] - loss: 0.018 - mae: 0.495 - mean_q: 0.266 - mean_eps: 0.848\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.4562\n",
      "366 episodes - episode_reward: -39.779 [-48.000, 35.076] - loss: 0.019 - mae: 0.515 - mean_q: 0.237 - mean_eps: 0.832\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.4688\n",
      "365 episodes - episode_reward: -40.261 [-47.990, 36.000] - loss: 0.019 - mae: 0.584 - mean_q: 0.235 - mean_eps: 0.816\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: -1.5016\n",
      "370 episodes - episode_reward: -40.578 [-48.000, 35.898] - loss: 0.019 - mae: 0.640 - mean_q: 0.228 - mean_eps: 0.800\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.4036\n",
      "362 episodes - episode_reward: -38.762 [-47.640, 35.672] - loss: 0.019 - mae: 0.713 - mean_q: 0.147 - mean_eps: 0.784\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: -1.5181\n",
      "376 episodes - episode_reward: -40.387 [-48.000, 32.940] - loss: 0.019 - mae: 0.676 - mean_q: 0.198 - mean_eps: 0.768\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: -1.4991\n",
      "373 episodes - episode_reward: -40.190 [-48.000, 35.653] - loss: 0.019 - mae: 0.780 - mean_q: 0.114 - mean_eps: 0.752\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.4239\n",
      "359 episodes - episode_reward: -39.666 [-48.000, 37.819] - loss: 0.019 - mae: 0.924 - mean_q: -0.059 - mean_eps: 0.736\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: -1.3743\n",
      "362 episodes - episode_reward: -37.949 [-48.000, 44.265] - loss: 0.018 - mae: 0.834 - mean_q: 0.000 - mean_eps: 0.720\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.4486\n",
      "372 episodes - episode_reward: -38.943 [-47.780, 41.171] - loss: 0.019 - mae: 0.895 - mean_q: -0.073 - mean_eps: 0.704\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.4202\n",
      "369 episodes - episode_reward: -38.492 [-47.980, 39.284] - loss: 0.018 - mae: 0.926 - mean_q: -0.121 - mean_eps: 0.688\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: -1.3962\n",
      "363 episodes - episode_reward: -38.464 [-47.980, 39.615] - loss: 0.019 - mae: 0.969 - mean_q: -0.100 - mean_eps: 0.672\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      " 7046/10000 [====================>.........] - ETA: 33s - reward: -1.4098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-31 17:32:24,684 - test2 - WARNING - The move 'outrage' was received in battle battle-gen8randombattle-45114 for your active pokemon Liepard. This move could not be added, but it might come from a special move such as copycat or me first. If that is not the case, please make sure there is an explanation for this behavior or report it if it is an error.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.4239\n",
      "363 episodes - episode_reward: -39.225 [-47.850, 34.781] - loss: 0.018 - mae: 1.072 - mean_q: -0.278 - mean_eps: 0.656\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.3200\n",
      "361 episodes - episode_reward: -36.561 [-47.850, 40.864] - loss: 0.018 - mae: 1.036 - mean_q: -0.259 - mean_eps: 0.640\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: -1.4776\n",
      "379 episodes - episode_reward: -38.974 [-47.960, 39.190] - loss: 0.018 - mae: 1.196 - mean_q: -0.268 - mean_eps: 0.624\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.4005\n",
      "368 episodes - episode_reward: -38.062 [-47.970, 40.381] - loss: 0.019 - mae: 1.270 - mean_q: -0.377 - mean_eps: 0.608\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.3762\n",
      "369 episodes - episode_reward: -37.298 [-48.000, 42.442] - loss: 0.018 - mae: 1.256 - mean_q: -0.327 - mean_eps: 0.592\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.4724\n",
      "381 episodes - episode_reward: -38.626 [-47.900, 38.378] - loss: 0.019 - mae: 1.167 - mean_q: -0.242 - mean_eps: 0.576\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.4021\n",
      "372 episodes - episode_reward: -37.712 [-48.000, 40.863] - loss: 0.019 - mae: 1.065 - mean_q: -0.229 - mean_eps: 0.560\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.3160\n",
      "362 episodes - episode_reward: -36.367 [-47.910, 47.253] - loss: 0.018 - mae: 1.069 - mean_q: -0.174 - mean_eps: 0.544\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.4246\n",
      "378 episodes - episode_reward: -37.663 [-48.000, 39.748] - loss: 0.019 - mae: 1.039 - mean_q: -0.207 - mean_eps: 0.528\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.4064\n",
      "377 episodes - episode_reward: -37.314 [-47.820, 40.021] - loss: 0.019 - mae: 1.315 - mean_q: -0.406 - mean_eps: 0.512\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.3664\n",
      "371 episodes - episode_reward: -36.837 [-48.000, 41.484] - loss: 0.019 - mae: 1.377 - mean_q: -0.463 - mean_eps: 0.496\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.3877\n",
      "364 episodes - episode_reward: -38.128 [-47.860, 45.921] - loss: 0.018 - mae: 1.347 - mean_q: -0.458 - mean_eps: 0.480\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 115s 11ms/step - reward: -1.4471\n",
      "365 episodes - episode_reward: -39.654 [-48.000, 37.407] - loss: 0.018 - mae: 1.217 - mean_q: -0.405 - mean_eps: 0.464\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      " 9817/10000 [============================>.] - ETA: 2s - reward: -1.3549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-31 17:57:26,531 - RandomPlayer 1 - WARNING - Trying to login as RandomPlayer 1, showdown returned  RandomPlayer 1@! - this might prevent future actions from this agent. Changing the agent's username might solve this problem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.3604\n",
      "371 episodes - episode_reward: -36.639 [-48.000, 45.339] - loss: 0.018 - mae: 1.529 - mean_q: -0.667 - mean_eps: 0.448\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: -1.3226\n",
      "363 episodes - episode_reward: -36.448 [-48.000, 44.009] - loss: 0.018 - mae: 1.416 - mean_q: -0.564 - mean_eps: 0.432\n",
      "\n",
      "Interval 37 (360000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: -1.3753\n",
      "363 episodes - episode_reward: -37.894 [-48.000, 38.023] - loss: 0.018 - mae: 1.366 - mean_q: -0.471 - mean_eps: 0.416\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.3533\n",
      "360 episodes - episode_reward: -37.590 [-48.000, 38.557] - loss: 0.018 - mae: 1.477 - mean_q: -0.507 - mean_eps: 0.400\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 115s 11ms/step - reward: -1.3234\n",
      "368 episodes - episode_reward: -35.955 [-47.990, 38.526] - loss: 0.018 - mae: 1.301 - mean_q: -0.362 - mean_eps: 0.384\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: -1.3364\n",
      "365 episodes - episode_reward: -36.639 [-48.000, 39.681] - loss: 0.018 - mae: 1.261 - mean_q: -0.306 - mean_eps: 0.368\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.3169\n",
      "358 episodes - episode_reward: -36.777 [-48.000, 44.821] - loss: 0.017 - mae: 1.422 - mean_q: -0.520 - mean_eps: 0.352\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -1.3157\n",
      "362 episodes - episode_reward: -36.335 [-47.940, 40.158] - loss: 0.018 - mae: 1.546 - mean_q: -0.598 - mean_eps: 0.336\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: -1.2768\n",
      "382 episodes - episode_reward: -33.428 [-47.900, 38.508] - loss: 0.019 - mae: 1.523 - mean_q: -0.503 - mean_eps: 0.320\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: -1.2344\n",
      "357 episodes - episode_reward: -34.580 [-47.770, 40.175] - loss: 0.018 - mae: 1.416 - mean_q: -0.481 - mean_eps: 0.304\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 115s 11ms/step - reward: -1.3280\n",
      "368 episodes - episode_reward: -36.079 [-48.000, 39.000] - loss: 0.018 - mae: 1.608 - mean_q: -0.660 - mean_eps: 0.288\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -1.2412\n",
      "361 episodes - episode_reward: -34.381 [-48.000, 44.210] - loss: 0.018 - mae: 1.368 - mean_q: -0.452 - mean_eps: 0.272\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: -1.1813\n",
      "367 episodes - episode_reward: -32.205 [-47.780, 43.932] - loss: 0.019 - mae: 1.230 - mean_q: -0.310 - mean_eps: 0.256\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 115s 11ms/step - reward: -1.2439\n",
      "361 episodes - episode_reward: -34.437 [-47.890, 44.339] - loss: 0.018 - mae: 1.450 - mean_q: -0.424 - mean_eps: 0.240\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: -1.2593\n",
      "381 episodes - episode_reward: -33.053 [-48.000, 45.000] - loss: 0.018 - mae: 1.779 - mean_q: -0.719 - mean_eps: 0.224\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: -1.1894\n",
      "done, took 5642.675 seconds\n"
     ]
    }
   ],
   "source": [
    "# Pick an agent to train against\n",
    "# await trainer.train(opponent=RandomPlayer(battle_format=\"gen8randombattle\"))\n",
    "\n",
    "await trainer.train(opponent=SimpleHeuristicsPlayer(battle_format=\"gen8randombattle\"), nb_steps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results against player: RandomPlayer 2\n",
      "DQN Evaluation: 91 victories out of 100 episodes\n",
      "Results against player: MaxBasePowerPlayer 1\n",
      "DQN Evaluation: 60 victories out of 100 episodes\n",
      "Results against player: SimpleHeuristicsPlayer 2\n",
      "DQN Evaluation: 11 victories out of 100 episodes\n"
     ]
    }
   ],
   "source": [
    "from poke_env.player.baselines import MaxBasePowerPlayer, SimpleHeuristicsPlayer\n",
    "\n",
    "opponents = [RandomPlayer(battle_format=\"gen8randombattle\"),\n",
    "            MaxBasePowerPlayer(battle_format=\"gen8randombattle\"),\n",
    "             SimpleHeuristicsPlayer(battle_format=\"gen8randombattle\")]\n",
    "\n",
    "await trainer.evaluate(opponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Evaluation: 0 victories out of 1 episodes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-65e2ac31b225>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     )\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbattle_human\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"meatout\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mready\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cancelled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mhandle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pk_env\\lib\\asyncio\\events.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_format_callback_source\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pk_env\\lib\\site-packages\\nest_asyncio.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(task, exc)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mcurr_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurr_tasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mstep_orig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurr_task\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pk_env\\lib\\asyncio\\tasks.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[1;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m                 \u001b[1;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nacha\\desktop\\pokecode\\pokebot\\pokebot\\trainers\\trainer.py\u001b[0m in \u001b[0;36mbattle_human\u001b[1;34m(self, opponent)\u001b[0m\n\u001b[0;32m     51\u001b[0m                            \u001b[0mopponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopponent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                            \u001b[0menv_algorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdqn_evaluation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                            env_algorithm_kwargs={\"dqn\": self.agent, \"nb_episodes\": 1})\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mSimpleDQNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nacha\\desktop\\pokecode\\pokebot\\pokebot\\trainers\\utils.py\u001b[0m in \u001b[0;36mplay_against_human\u001b[1;34m(player, env_algorithm, opponent, env_algorithm_kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_new_battle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mloop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlaunch_battles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\pk_env\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pk_env\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test agent against yourself\n",
    "\n",
    "trainer.player = BotPlayer(\n",
    "    player_configuration=PlayerConfiguration(\"JoeNextLine\", None),\n",
    "#     log_level=10,\n",
    "    state_engine=HeuristicStateEngine()\n",
    "    )\n",
    "\n",
    "asyncio.get_event_loop().run_until_complete(trainer.battle_human(\"meatout\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save current player\n",
    "\n",
    "trainer.agent.save_weights(\"../data/agents/evenlesserbad.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load agent\n",
    "\n",
    "trainer.agent.load_weights(\"../data/agents/bad.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pkmn)",
   "language": "python",
   "name": "pk_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
